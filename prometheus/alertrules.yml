groups:
  - name: system_alerts
    interval: 30s
    rules:
      - alert: InstanceDown
        expr: up{job!="alertmanager"} == 0
        for: 5m
        annotations:
          summary: "Instance {{ $labels.instance }} is down"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for 5+ minutes."

      - alert: HighCPUUsage
        expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: > 80%)"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85
        for: 5m
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanizePercentage }} (threshold: > 85%)"

      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes) < 0.15
        for: 5m
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Available disk space is {{ $value | humanizePercentage }} (threshold: < 15%)"

      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes) < 0.10
        for: 2m
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Available disk space is {{ $value | humanizePercentage }} (threshold: < 10%)"

      - alert: HighDiskIO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.8
        for: 5m
        annotations:
          summary: "High disk I/O on {{ $labels.instance }}"
          description: "Disk I/O time is {{ $value | humanize }}s (threshold: > 0.8)"

      - alert: NetworkErrors
        expr: rate(node_network_transmit_errs_total[5m]) + rate(node_network_receive_errs_total[5m]) > 0
        for: 5m
        annotations:
          summary: "Network errors on {{ $labels.instance }}"
          description: "Network error rate is {{ $value | humanize }} errors/sec"

  - name: prometheus_alerts
    interval: 30s
    rules:
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        annotations:
          summary: "Prometheus configuration reload failed"
          description: "Prometheus failed to reload configuration at {{ $value | formatTime }}"

      - alert: PrometheusTooManyRestarts
        expr: changes(prometheus_tsdb_startups_total[15m]) > 2
        for: 5m
        annotations:
          summary: "Prometheus has restarted too many times"
          description: "Prometheus has restarted {{ $value | humanize }} times in 15 minutes"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 5m
        annotations:
          summary: "AlertManager is down"
          description: "AlertManager instance has been down for 5+ minutes"
        expr: sum(rate(http_requests_total[30m]))

      # p50 and p99 latency (additional windows)
      - record: slo:latency_p50:5m
        expr: histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))

      - record: slo:latency_p99:6h
        expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[6h])) by (le))

      # Additional multi-window error metrics
      - record: slo:error_rate:6h
        expr: |
          (
            sum(rate(http_requests_total{job="app",status=~"5.."}[6h])) 
            / 
            sum(rate(http_requests_total{job="app"}[6h]))
          ) * 100

      - record: slo:error_rate:30d
        expr: |
          (
            sum(increase(http_requests_total{job="app",status=~"5.."}[30d])) 
            / 
            sum(increase(http_requests_total{job="app"}[30d]))
          ) * 100



  - name: traditional_alerts
    interval: 15s
    rules:
      - alert: InstanceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Instance {{ $labels.instance }} down"
          description: "Instance {{ $labels.instance }} has been down for more than 2 minutes."

      - alert: HighCPUUsage
        expr: (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}."

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}."

      - alert: HighDiskUsage
        expr: (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes) * 100 < 15
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High disk usage on {{ $labels.device }}"
          description: "Disk {{ $labels.device }} is {{ $value }}% full."

      - alert: HighIOWait
        expr: avg by (instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) * 100 > 20
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High I/O wait on {{ $labels.instance }}"
          description: "I/O wait is {{ $value }}% on {{ $labels.instance }}."

      - alert: NetworkErrors
        expr: increase(node_network_transmit_errs_total[5m]) > 0 or increase(node_network_receive_errs_total[5m]) > 0
        for: 2m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "Network errors on {{ $labels.instance }}"
          description: "Network errors detected on {{ $labels.device }}."

      - alert: PrometheusConfigError
        expr: prometheus_config_last_reload_successful == 0
        for: 1m
        labels:
          severity: critical
          service: prometheus
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus failed to reload its configuration."

      - alert: PrometheusRestartCount
        expr: increase(prometheus_tsdb_reloads_total[1h]) > 5
        for: 5m
        labels:
          severity: warning
          service: prometheus
        annotations:
          summary: "Prometheus restarts detected"
          description: "Prometheus has restarted {{ $value }} times in the last hour."

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: critical
          service: alertmanager
        annotations:
          summary: "AlertManager is down"
          description: "AlertManager has been down for more than 2 minutes."

  - name: slo_alerts
    interval: 15s
    rules:
      - alert: SLOBurnRateFast5m
        expr: slo:burn_rate:5m > 14.4 and slo:burn_rate:30m > 14.4
        for: 2m
        labels:
          severity: critical
          slo_alert: "true"
          burn_type: fast
        annotations:
          summary: "SLO error budget burning fast (5m window)"
          description: "Burn rate is {{ $value }}x over 5m. Error budget exhaustion in ~1 hour."

      - alert: SLOBurnRateSlow1h
        expr: slo:burn_rate:1h > 6 and slo:burn_rate:6h > 6
        for: 15m
        labels:
          severity: warning
          slo_alert: "true"
          burn_type: slow
        annotations:
          summary: "SLO error budget burning slowly (1h window)"
          description: "Burn rate is {{ $value }}x over 1h. Error budget exhaustion in ~1 week."

      - alert: LatencyBreach
        expr: slo:latency_p95:5m > 0.5
        for: 5m
        labels:
          severity: warning
          slo_alert: "true"
        annotations:
          summary: "p95 latency breach (target: < 500ms)"
          description: "p95 latency is {{ $value }}s, exceeding SLO target of 500ms."

      - alert: ErrorRateBreach
        expr: slo:error_rate:5m > 0.1
        for: 5m
        labels:
          severity: warning
          slo_alert: "true"
        annotations:
          summary: "Error rate breach (target: < 0.1%)"
          description: "Error rate is {{ $value }}%, exceeding SLO target of 0.1%."

      - alert: ErrorBudgetExhausted
        expr: slo:error_budget_remaining:30d <= 0
        for: 1m
        labels:
          severity: critical
          slo_alert: "true"
        annotations:
          summary: "30-day error budget exhausted"
          description: "Monthly error budget has been fully consumed. SLO is now at risk."

      - alert: SSLCertificateExpiring
        expr: (ssl_cert_expiry_days < 30) and (ssl_cert_expiry_days > 0)
        for: 1h
        labels:
          severity: warning
          slo_alert: "true"
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate expires in {{ $value }} days."

  - name: cost_alerts
    interval: 15m
    rules:
      - alert: DailyCostBudgetExceeded
        expr: increase(aws_cost_total[1d]) > 2
        for: 10m
        labels:
          severity: warning
          cost_alert: "true"
        annotations:
          summary: "Daily cost budget exceeded (target: $2/day)"
          description: "Today's cost is ${{ $value }}, exceeding daily budget of $2."

      - alert: MonthlyCostBudgetExceeded
        expr: increase(aws_cost_total[30d]) > 20
        for: 1h
        labels:
          severity: warning
          cost_alert: "true"
        annotations:
          summary: "Monthly cost budget exceeded (target: $20/mo)"
          description: "Monthly cost is ${{ $value }}, exceeding budget of $20."

      - alert: CostSpikeLarge
        expr: |
          increase(aws_cost_total[1d]) > 
          (
            avg_over_time(increase(aws_cost_total[1d])[7d:1d]) * 1.2
          )
        for: 30m
        labels:
          severity: warning
          cost_alert: "true"
        annotations:
          summary: "Unusual cost spike detected (>20%)"
          description: "Daily cost increased by more than 20% from 7-day average."

      - alert: CostForecastWarning
        expr: increase(aws_cost_total[7d]) * 4.3 > 25
        for: 2h
        labels:
          severity: info
          cost_alert: "true"
        annotations:
          summary: "Projected monthly cost forecast high"
          description: "Current burn rate projects to ${{ $value }}/month, above budget."

      - alert: UnusedInstanceWarning
        expr: |
          (
            avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) 
            by (instance)
          ) < 0.05
        for: 1h
        labels:
          severity: info
          cost_alert: "true"
        annotations:
          summary: "Instance {{ $labels.instance }} has low CPU utilization"
          description: "Average CPU usage is {{ $value | humanizePercentage }} â€” consider downsizing."

      - alert: AvailabilityBreach
        expr: slo:availability:5m < 99.9
        for: 5m
        labels:
          severity: warning
          slo_alert: "true"
        annotations:
          summary: "Service availability below SLO target"
          description: "Availability is {{ $value }}%, below SLO target of 99.9%."

      - alert: PrometheusHighMemory
        expr: prometheus_tsdb_symbol_table_size_bytes > 1073741824
        for: 5m
        labels:
          severity: warning
          service: prometheus
        annotations:
          summary: "Prometheus high memory usage"
          description: "Prometheus symbol table is {{ $value | humanize }}B, consider increasing retention or diskspace."

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          service: grafana
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 2 minutes."

      - alert: NodeExporterDown
        expr: up{job="node_exporter"} == 0
        for: 2m
        labels:
          severity: warning
          service: node_exporter
        annotations:
          summary: "Node Exporter is down on {{ $labels.instance }}"
          description: "Node Exporter on {{ $labels.instance }} has been down for more than 2 minutes."

      - alert: CostExporterDown
        expr: up{job="cost_exporter"} == 0
        for: 5m
        labels:
          severity: info
          cost_alert: "true"
        annotations:
          summary: "Cost Exporter is down"
          description: "Cost Exporter has been down for more than 5 minutes. Cost metrics will not be collected."
